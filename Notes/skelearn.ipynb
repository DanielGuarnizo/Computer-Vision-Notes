{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color= yellow> - MLP = MULTI LAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# MNIST dataset containing handiwritten digits (from 0 to 9)\n",
    "# each digit is a 28x28 pixels (greyscale = single channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielguarnizo/anaconda3/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD THE DATA SET\n",
    "X,y = fetch_openml('mnist_784', return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielguarnizo/anaconda3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5428163265306123\n"
     ]
    }
   ],
   "source": [
    "# create train and test dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.7)\n",
    "\n",
    "# create the model \n",
    "model = MLPClassifier(hidden_layer_sizes=(20, ), solver=\"sgd\")# how many hiden layers do we want between input and output layer\n",
    "    #@ the number of rows are the nubers of newrounds in the layier \n",
    "    #@ solver are the algorithm used to do the forward pass\n",
    "\n",
    "# train the model \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# test the model \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# the sccuracy of out model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e53101bd0d66bc69c5af7be29741f8f2ce3039903f322d3356d2f6a30c89c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
