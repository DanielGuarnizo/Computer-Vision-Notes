{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/danielguarnizo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/danielguarnizo/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielguarnizo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielguarnizo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/danielguarnizo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"tagsets\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=yellow> Preprocessing NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hello there, I'm Daniel\"] ['Hello', 'there', ',', 'I', \"'m\", 'Daniel']\n"
     ]
    }
   ],
   "source": [
    "# example_string= \"all the speed he took, all turn he'd taken and the corners in night City...\"\n",
    "example_string = \"Hello there, I'm Daniel\"\n",
    "sent_t = sent_tokenize(example_string)\n",
    "sent_w = word_tokenize(example_string)\n",
    "print(sent_t,sent_w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'leviosa', ',', 'not', 'leviosaa'] [\"'s\", 'leviosa', ',', 'leviosaa']\n"
     ]
    }
   ],
   "source": [
    "# are words commonly used  \n",
    "# content word that give us information about the sentence, without them the sentence somehow loss meaning \n",
    "# context word \n",
    "\n",
    "quote = \"It's leviosa, not leviosaa\"\n",
    "word_in_quote = word_tokenize(quote)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_word = []\n",
    "\n",
    "for word in word_in_quote:\n",
    "    if word.casefold() not in stop_words: # with casefold we are ignoring if the word is in upper or lower case\n",
    "        filtered_word.append(word)\n",
    "print(word_in_quote, filtered_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'crew', 'of', 'uss', 'discoveri', 'dicov', 'mani', 'discoveri', '.', 'discov', 'is', 'what', 'explor', 'do']\n"
     ]
    }
   ],
   "source": [
    "# stemming is the process of producing morphological variance or a root/base word, it means, given a root word \"retrive\"\n",
    "# adding difference parts of words like  “retrieval”, “retrieved”, “retrieves” it change the word meaning of the root word\n",
    "#@ key idea:  technique that is used to reduce words to their base form, also known as the root form.\n",
    "\n",
    "# overstemming false positive:This means that two unrelated words are reduced to the same stem, even though they have different meanings.\n",
    "# understemming false negative: This means that two related words are not reduced to the same stem, even though they have similar meanings.\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "#\n",
    "string_to_stem = \"the crew of USS discovery dicovered many discoveries. Discovering is what explorers do\"\n",
    "\n",
    "# tokenizing the string\n",
    "words = word_tokenize(string_to_stem)\n",
    "\n",
    "# applying the stemmer on the extracted words \n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Part of speech (Pos) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('computer', 'NN'), ('science', 'NN'), ('is', 'VBZ'), ('no', 'DT'), ('more', 'RBR'), ('about', 'IN'), ('computers', 'NNS'), ('than', 'IN'), ('astronomy', 'NN'), ('is', 'VBZ'), ('about', 'IN'), ('telescopes', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@ Neureal network pre-training in given a set of words and it taggs this words \n",
    "dijkstra = \"computer science is no more about computers than astronomy is about telescopes.\"\n",
    "\n",
    "# tokeninzing the quote \n",
    "words = word_tokenize(dijkstra)\n",
    "\n",
    "# pos tagging on the quote \n",
    "result = nltk.pos_tag(words)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Tags set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$: dollar\n",
    "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
    "'': closing quotation mark\n",
    "    ' ''\n",
    "(: opening parenthesis\n",
    "    ( [ {\n",
    "): closing parenthesis\n",
    "    ) ] }\n",
    ",: comma\n",
    "    ,\n",
    "--: dash\n",
    "    --\n",
    ".: sentence terminator\n",
    "    . ! ?\n",
    ":: colon or ellipsis\n",
    "    : ; ...\n",
    "CC: conjunction, coordinating\n",
    "    & 'n and both but either et for less minus neither nor or plus so\n",
    "    therefore times v. versus vs. whether yet\n",
    "CD: numeral, cardinal\n",
    "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
    "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
    "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
    "DT: determiner\n",
    "    all an another any both del each either every half la many much nary\n",
    "    neither no some such that the them these this those\n",
    "EX: existential there\n",
    "    there\n",
    "FW: foreign word\n",
    "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
    "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
    "    terram fiche oui corporis ...\n",
    "IN: preposition or conjunction, subordinating\n",
    "    astride among uppon whether out inside pro despite on by throughout\n",
    "    below within for towards near behind atop around if like until below\n",
    "    next into if beside ...\n",
    "JJ: adjective or numeral, ordinal\n",
    "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
    "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
    "    multilingual multi-disciplinary ...\n",
    "JJR: adjective, comparative\n",
    "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
    "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
    "    cozier creamier crunchier cuter ...\n",
    "JJS: adjective, superlative\n",
    "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
    "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
    "    dearest deepest densest dinkiest ...\n",
    "LS: list item marker\n",
    "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
    "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
    "    two\n",
    "MD: modal auxiliary\n",
    "    can cannot could couldn't dare may might must need ought shall should\n",
    "    shouldn't will would\n",
    "NN: noun, common, singular or mass\n",
    "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
    "    investment slide humour falloff slick wind hyena override subhumanity\n",
    "    machinist ...\n",
    "NNP: noun, proper, singular\n",
    "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
    "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
    "    Shannon A.K.C. Meltex Liverpool ...\n",
    "NNPS: noun, proper, plural\n",
    "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
    "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
    "    Apache Apaches Apocrypha ...\n",
    "NNS: noun, common, plural\n",
    "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
    "    divestitures storehouses designs clubs fragrances averages\n",
    "    subjectivists apprehensions muses factory-jobs ...\n",
    "PDT: pre-determiner\n",
    "    all both half many quite such sure this\n",
    "POS: genitive marker\n",
    "    ' 's\n",
    "PRP: pronoun, personal\n",
    "    hers herself him himself hisself it itself me myself one oneself ours\n",
    "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
    "PRP$: pronoun, possessive\n",
    "    her his mine my our ours their thy your\n",
    "RB: adverb\n",
    "    occasionally unabatingly maddeningly adventurously professedly\n",
    "    stirringly prominently technologically magisterially predominately\n",
    "    swiftly fiscally pitilessly ...\n",
    "RBR: adverb, comparative\n",
    "    further gloomier grander graver greater grimmer harder harsher\n",
    "    healthier heavier higher however larger later leaner lengthier less-\n",
    "    perfectly lesser lonelier longer louder lower more ...\n",
    "RBS: adverb, superlative\n",
    "    best biggest bluntest earliest farthest first furthest hardest\n",
    "    heartiest highest largest least less most nearest second tightest worst\n",
    "RP: particle\n",
    "    aboard about across along apart around aside at away back before behind\n",
    "    by crop down ever fast for forth from go high i.e. in into just later\n",
    "    low more off on open out over per pie raising start teeth that through\n",
    "    under unto up up-pp upon whole with you\n",
    "SYM: symbol\n",
    "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
    "TO: \"to\" as preposition or infinitive marker\n",
    "    to\n",
    "UH: interjection\n",
    "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
    "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
    "    man baby diddle hush sonuvabitch ...\n",
    "VB: verb, base form\n",
    "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
    "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
    "    boost brace break bring broil brush build ...\n",
    "VBD: verb, past tense\n",
    "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
    "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
    "    speculated wore appreciated contemplated ...\n",
    "VBG: verb, present participle or gerund\n",
    "    telegraphing stirring focusing angering judging stalling lactating\n",
    "    hankerin' alleging veering capping approaching traveling besieging\n",
    "    encrypting interrupting erasing wincing ...\n",
    "VBN: verb, past participle\n",
    "    multihulled dilapidated aerosolized chaired languished panelized used\n",
    "    experimented flourished imitated reunifed factored condensed sheared\n",
    "    unsettled primed dubbed desired ...\n",
    "VBP: verb, present tense, not 3rd person singular\n",
    "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
    "    appear tend stray glisten obtain comprise detest tease attract\n",
    "    emphasize mold postpone sever return wag ...\n",
    "VBZ: verb, present tense, 3rd person singular\n",
    "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
    "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
    "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
    "WDT: WH-determiner\n",
    "    that what whatever which whichever\n",
    "WP: WH-pronoun\n",
    "    that what whatever whatsoever which who whom whosoever\n",
    "WP$: WH-pronoun, possessive\n",
    "    whose\n",
    "WRB: Wh-adverb\n",
    "    how however whence whenever where whereby whereever wherein whereof why\n",
    "``: opening quotation mark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst bad\n"
     ]
    }
   ],
   "source": [
    "# lemmatizing as stemming get to the root of a word but in this case it takes in acount the meaning of the word\n",
    "lemmatizer = WordNetLemmatizer() # we create an instance of the algorith without putting any parameters \n",
    "\n",
    "result = lemmatizer.lemmatize(\"worst\")\n",
    "result1 = lemmatizer.lemmatize(\"worst\", pos=\"a\") # we help to the lemmatizer that the word is not an NN but and adjective word\n",
    "\n",
    "print(result, result1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('dangerous', 'JJ'), ('business', 'NN'), (',', ','), ('Frodo', 'NNP'), (',', ','), ('going', 'VBG'), ('out', 'RP'), ('your', 'PRP$'), ('door', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m chunk_parser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mRegexpChunkParser(my_string) \u001b[39m# parameter is the template we create \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# let's see how the chunker words \u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m tree \u001b[39m=\u001b[39m chunk_parser\u001b[39m.\u001b[39;49mparse(lotr_pos_tagging)\n\u001b[1;32m     23\u001b[0m tree\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/chunk/regexp.py:1092\u001b[0m, in \u001b[0;36mRegexpChunkParser.parse\u001b[0;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trace_apply(chunkstr, verbose)\n\u001b[1;32m   1091\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1092\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_notrace_apply(chunkstr)\n\u001b[1;32m   1094\u001b[0m \u001b[39m# Use the chunkstring to create a chunk structure.\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39mreturn\u001b[39;00m chunkstr\u001b[39m.\u001b[39mto_chunkstruct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_label)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/chunk/regexp.py:1052\u001b[0m, in \u001b[0;36mRegexpChunkParser._notrace_apply\u001b[0;34m(self, chunkstr)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39mApply each rule of this ``RegexpChunkParser`` to ``chunkstr``, in\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39mturn.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[39m:rtype: None\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[39mfor\u001b[39;00m rule \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rules:\n\u001b[0;32m-> 1052\u001b[0m     rule\u001b[39m.\u001b[39;49mapply(chunkstr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "lotr = \"It's a dangerous business, Frodo, going out your door.\"\n",
    "\n",
    "# let's tokenize for doing pos\n",
    "words_in_lotr = word_tokenize(lotr)\n",
    "\n",
    "\n",
    "# let's tag the words \n",
    "lotr_pos_tagging = nltk.pos_tag(words_in_lotr)\n",
    "print(lotr_pos_tagging)\n",
    "\n",
    "\n",
    "# template how the strings of the bigger string should be \n",
    "# NP: noun phrase\n",
    "# DT: determiner, ? --> this determiner is optional (quantitative symbols, indicates how many DT can be in the phrase)\n",
    "# JJ: adjective, * --> any number \n",
    "# NN: noun\n",
    "my_string = \"NP: {<DT>?<JJ>*<NN>}\" # are the tags or the pos tagging \n",
    "\n",
    "chunk_parser = nltk.RegexpChunkParser(my_string) # parameter is the template we create \n",
    "\n",
    "# let's see how the chunker words \n",
    "tree = chunk_parser.parse(lotr_pos_tagging)\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e53101bd0d66bc69c5af7be29741f8f2ce3039903f322d3356d2f6a30c89c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
